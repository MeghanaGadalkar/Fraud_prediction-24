# -*- coding: utf-8 -*-
"""Final_FraudPredictionProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ATJo2odehocvksGDGXaY_7rqBIh6alLB

**Problem Statement :**

**Build a predictive model to detect fraudulent claims in the consumer electronics industry based on the provided dataset.**

**Group members:**

               1)Darshan wagh              -333  

               2)Meghana Gadalkar          -335

               3)Kaustubh Chaudhari        -33

               4)Tushar suryawanshi        -33
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot
from sklearn import preprocessing

"""**Understanding data**"""

data=pd.read_csv("/content/fraud.csv")
data

data.head()

data.tail()

data.shape

data.describe()

data['State'].value_counts()

data.columns

data.nunique()

data['Area'].value_counts()

data.min()

data.max()

"""**Cleaning dataset**"""

data.isnull()

pf=data.drop( ['AC_1001_Issue','AC_1002_Issue','AC_1003_Issue','TV_2001_Issue','TV_2002_Issue','TV_2003_Issue'],axis=1)
pf

#checking outliers
correlation=pf.corr()

sns.heatmap(correlation,xticklabels=correlation.columns,yticklabels=correlation.columns,annot=True)

#pairplot can be shows the relation between catergorical ,continous values
import seaborn as sns
import matplotlib.pyplot as plt
# Assuming 'pf' is your DataFrame and 'Fraud' is one of its columns
sns.pairplot(pf)
plt.show()

#scatter plot shows between two numerical column
import seaborn as sns

sns.relplot(x='Claim_Value', y='Product_Age', hue='Product_type', data=pf)

#for categorical columns

sns.countplot(pf['Region'])

sns.countplot(pf['City'])

sns.countplot(pf['State'])

pf['Product_category'].value_counts().plot(kind='pie',autopct='%.2f')

pf['Call_details'].value_counts().plot(kind='bar')

sns.relplot(x='Claim_Value', y='Product_Age', hue='Consumer_profile', data=pf)

sns.distplot(pf['Call_details'],bins=3)

sns.catplot(x='Claim_Value',kind='box',data=pf)

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'pf' is your DataFrame
sns.countplot(x='Region', hue='Purchased_from', data=pf)
plt.title('Count of Stores by Region and Purchase Source')
plt.xlabel('Region')
plt.ylabel('Count')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Set the figure size
plt.figure(figsize=(10, 6))

# Create the count plot
sns.countplot(x='Region', hue='Purchased_from', data=pf)

# Rotate x-axis labels
plt.xticks(rotation=45)

plt.title('Count of Stores by Region and Purchase Source')
plt.xlabel('Region')
plt.ylabel('Count')
plt.show()

pf['Fraud'].value_counts()

class_balance = pf['Fraud'].value_counts(normalize=True)
class_balance

"""**Target variable is a 'fraud' in this column one class dominates the other (zero class has significantly higher proprtion and one class has lower proportion) then dataset is imbalance.**

**Five methods to handle imbalance data:-**
1)Undersampling:
   Due to undersampling many data thrown away so it is not good approach to hanadle imbalance data.
2)Oversampling:
  Duplicating samples not better way
3)SMOTE (Synthetic minority Over-sampling technique):
4)Ensemble method:
5)Focal Loss:
"""

#we have to balance it

import pandas as pd
ss=pd.read_csv('/content/fraud.csv')
ss.head()

ss['Fraud'].value_counts()

ss.shape

x=ss.iloc[:,3:-1]
y=ss.Fraud
x.head()

from sklearn.preprocessing import LabelEncoder
enc=LabelEncoder()
mm=x.loc[:,['Area', 'City', 'Consumer_profile', 'Product_category', 'Product_type',
       'AC_1001_Issue', 'AC_1002_Issue', 'AC_1003_Issue', 'TV_2001_Issue',
       'TV_2002_Issue', 'TV_2003_Issue', 'Claim_Value', 'Service_Centre',
       'Product_Age', 'Purchased_from', 'Call_details', 'Purpose']]=x.loc[:,['Area', 'City', 'Consumer_profile', 'Product_category', 'Product_type',
       'AC_1001_Issue', 'AC_1002_Issue', 'AC_1003_Issue', 'TV_2001_Issue',
       'TV_2002_Issue', 'TV_2003_Issue', 'Claim_Value', 'Service_Centre',
       'Product_Age', 'Purchased_from', 'Call_details', 'Purpose']].apply(enc.fit_transform)
mm

# Print out the columns of the DataFrame x
print(x.columns)

from sklearn.neighbors import KNeighborsClassifier
x_train, x_test, y_train, y_test = train_test_split(mm, y, test_size=0.3, random_state=10)

# Initialize and train KNeighborsClassifier model
model = KNeighborsClassifier()

from sklearn.neighbors import KNeighborsClassifier
model=KNeighborsClassifier()
model.fit(x_train,y_train)

y_predict = model.predict(x_test)

y_predict = model.predict(x_test)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_test ,y_predict))



pd.crosstab(y_test ,y_predict)

#Hence,in above 103 outoff 100 are correctely classified and outoff 5 all 5 are not correctly classified

100/103

0/5

3/100

#SMOTE technique

!pip install imblearn

from imblearn.over_sampling import SMOTE

smote = SMOTE()  # Initialize SMOTE

x_train_smote, y_train_smote = smote.fit_resample(x_train.astype('float'), y_train)

from collections import Counter
print("Before SMOTE:",Counter(y_train))
print("After SMOTE:",Counter(y_train_smote))

#In this way,using smote  ALL have same no of output 220 0's and 220 1's

!pip install pandas-profiling

"""**Normalization**"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data.drop('Fraud', axis=1),
                                                    data['Fraud'],
                                                    test_size=0.3,
                                                    random_state=0)

X_train.shape, X_test.shape

# Assuming 'df' is your DataFrame containing the dataset
X = pf.drop('Fraud', axis=1)  # Drop the 'Fraud' column from the features
y = pf['Fraud']  # Target variable is 'Fraud'

# Display the shapes of X and y
print("Shape of X:", X.shape)
print("Shape of y:", y.shape)

X_train

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Assuming X contains your features and y contains your target variable
# X should contain both numerical and categorical columns

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Convert categorical columns to numerical using LabelEncoder
categorical_columns = ['Region', 'State', 'Area', 'City', 'Consumer_profile',
                       'Product_category', 'Product_type', 'Purchased_from',
                       'Call_details', 'Purpose']

for col in categorical_columns:
    X[col] = label_encoder.fit_transform(X[col])

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the train set
scaler.fit(X_train)

# Transform train and test sets
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

scaler.mean_

X_train

X_train_scaled

np.round(X_train.describe(), 1)

X_train_scaled=pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled=pd.DataFrame(X_test_scaled, columns=X_test.columns)

np.round(X_train_scaled.describe(), 1)

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))

# before scaling
ax1.set_title('Before Scaling')
sns.kdeplot(X_train['Product_Age'], ax=ax1)
sns.kdeplot(X_train['Claim_Value'], ax=ax1)

# after scaling
ax2.set_title('After Standard Scaling')
sns.kdeplot(X_train_scaled['Product_Age'], ax=ax2)
sns.kdeplot(X_train_scaled['Claim_Value'], ax=ax2)
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))

# before scaling
ax1.set_title('Before Scaling')
sns.kdeplot(X_train['Call_details'], ax=ax1)
sns.kdeplot(X_train['Purpose'], ax=ax1)

# after scaling
ax2.set_title('After Standard Scaling')
sns.kdeplot(X_train_scaled['Call_details'], ax=ax2)
sns.kdeplot(X_train_scaled['Purpose'], ax=ax2)
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))

# before scaling
ax1.set_title('Before Scaling')
sns.scatterplot(X_train['Call_details'], ax=ax1)
sns.scatterplot(X_train['Purpose'], ax=ax1)

# after scaling
ax2.set_title('After Standard Scaling')
sns.scatterplot(X_train_scaled['Call_details'], ax=ax2)
sns.scatterplot(X_train_scaled['Purpose'], ax=ax2)
plt.show()

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr_scaled = LogisticRegression()

lr.fit(X_train,y_train)
lr_scaled.fit(X_train_scaled,y_train)

y_pred = lr.predict(X_test)
y_pred_scaled = lr_scaled.predict(X_test_scaled)

from sklearn.metrics import accuracy_score

print("Actual",accuracy_score(y_test,y_pred))
print("Scaled",accuracy_score(y_test,y_pred_scaled))

"""**One Hot Encoding**"""

unique_categories =pf['Area'].unique()
print("Unique categories in the column:",unique_categories)

unique_categories =pf['Purchased_from'].unique()
print("Unique categories in the column:",unique_categories)

unique_categories =pf['Purpose'].unique()
print("Unique categories in the column:",unique_categories)

from sklearn.preprocessing import OrdinalEncoder

X_train

oe = OrdinalEncoder(categories=[['Urban','Rural'],['Complaint','Claim','Other']])

from sklearn.preprocessing import OrdinalEncoder

# Assuming X_train contains your training data

# Initialize OrdinalEncoder with handle_unknown='use_encoded_value' and unknown_value=-1
oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)

# Fit the encoder to the training data
oe.fit(X_train)

# Transform the training data
X_train_encoded = oe.transform(X_train)

X_train = oe.transform(X_train)

X_train

oe.categories_

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

le.fit(y_train)

le.classes_

y_train = le.transform(y_train)
y_test = le.transform(y_test)

y_train

#One_hot_encoding using sklearn

data.head()

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(data.iloc[:,0:4],data.iloc[:,-1],test_size=0.2,random_state=2)

X_train.head()

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(drop='first',sparse=False,dtype=np.int32)

X_train_new = ohe.fit_transform(X_train[['State','Area']])

X_test_new = ohe.transform(X_test[['State','Area']])

X_train_new.shape

"""**Random Forest**"""

import pandas as pd

def find_categorical_columns(data):
    categorical_columns = data.select_dtypes(include=['object']).columns.tolist()
    return categorical_columns

# Example usage:
# Assuming 'df' is your DataFrame
categorical_columns = find_categorical_columns(data)
print("Categorical columns:", categorical_columns)

import pandas as pd

# Assuming 'df' is your DataFrame containing categorical columns
df_encoded = pd.get_dummies(data)
df_encoded

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data.drop('Fraud', axis=1),
                                                    data['Fraud'],
                                                    test_size=0.3,
                                                    random_state=0)

X_train.shape, X_test.shape

from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier()
rf.fit(X_train,y_train)

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()


# Fit the pipeline
model.fit(preprocessor.fit_transform(X_train), y_train)